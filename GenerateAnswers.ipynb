{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BPNr3FrYe8x"
   },
   "source": [
    "## Use powerful LLM to generate synthetic answer for a given question - context pair.\n",
    "\n",
    "This will create a dataset of question - context - answer triplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WtW9_Bm98oWR"
   },
   "outputs": [],
   "source": [
    "# provide project root path\n",
    "# ProjectRoot = \"/content/drive/MyDrive/UMich Capstone/NoteBooks/\"\n",
    "ProjectRoot = \"/home/sangram/Tutorbot_capstone/git_hub/Tutorbot/\"\n",
    "DatasetRoot = ProjectRoot + \"Dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "96SM0P5f8oWT"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import transformers\n",
    "except ImportError:\n",
    "    !pip install transformers\n",
    "\n",
    "try:\n",
    "    import accelerate\n",
    "except ImportError:\n",
    "    !pip install accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 506,
     "status": "ok",
     "timestamp": 1722005036821,
     "user": {
      "displayName": "Tung Nguyen",
      "userId": "00606808731810408814"
     },
     "user_tz": -420
    },
    "id": "bSTqf6Fn8oWU",
    "outputId": "c6766e70-7f74-4623-d5b6-83d681d31521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.43.4\n"
     ]
    }
   ],
   "source": [
    "from transformers import __version__\n",
    "print(__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qE3wZdl-8oWV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "executionInfo": {
     "elapsed": 503,
     "status": "error",
     "timestamp": 1722005069360,
     "user": {
      "displayName": "Tung Nguyen",
      "userId": "00606808731810408814"
     },
     "user_tz": -420
    },
    "id": "TtNJOTVz8oWW",
    "outputId": "b4fb2a0f-5601-49a4-f1cf-cea996f73f58"
   },
   "outputs": [],
   "source": [
    "# load context and question test set which was created by doc2query\n",
    "train_df = pd.read_csv(DatasetRoot + 'q_a_trainset.csv')\n",
    "test_df = pd.read_csv(DatasetRoot + 'q_a_testset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KnHFMsYA8oWW"
   },
   "outputs": [],
   "source": [
    "# loading full article from json file\n",
    "with open(DatasetRoot + 'raw_knowledge.json', 'r') as f:\n",
    "    raw_text_json = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UkpYuTh78oWX",
    "outputId": "4fba1f0c-e32e-4620-9944-3b15c132d3f6"
   },
   "outputs": [],
   "source": [
    "raw_df = pd.DataFrame(list(raw_text_json.items()), columns=['raw_para_id', 'raw_text'])\n",
    "raw_df['raw_para_id'] = raw_df['raw_para_id'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "p93cTgFe8oWY"
   },
   "outputs": [],
   "source": [
    "# create dataframe of raw, summarized paragraphs and question\n",
    "train_df = train_df.merge(raw_df, left_on='raw_para_id', right_on='raw_para_id', how='left')\n",
    "test_df = test_df.merge(raw_df, left_on='raw_para_id', right_on='raw_para_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "muFuSpJu8oWZ",
    "outputId": "0d4cb257-4d57-497f-d175-7c04d02a36c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device(\"cuda\")\n",
    "    print(\"CUDA is available!!\")\n",
    "else:\n",
    "    raise RuntimeError(\"CUDA is not available!! LLM cannot run, rerun with GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xs8BIbHf8oWZ"
   },
   "outputs": [],
   "source": [
    "# Provide Huggingface Login token below to leverage powerful LLMs\n",
    "os.environ['HUGGINGFACE_TOKEN'] = 'Add your token'\n",
    "if os.environ['HUGGINGFACE_TOKEN'] == 'Add your token':\n",
    "    raise ValueError(\"Token not provided\")\n",
    "    \n",
    "token = os.environ.get('HUGGINGFACE_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ce577wKY8oWb",
    "outputId": "55a9cbe6-8344-4b31-f145-5207b2d15bf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/sangram/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token $token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBmBBZnR8oWb"
   },
   "source": [
    "### Generate ground truths\n",
    "\n",
    "Note: Huggingface models are cached under /home/sangram/.cache/huggingface/hub/models--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "C2dM-lrj8oWb",
    "outputId": "4e896daa-0ba3-4c29-b067-cb5eef94006b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug  7 18:52:02 2024       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  Tesla V100-PCIE-16GB           On  |   00000000:3B:00.0 Off |                    0 |\r\n",
      "| N/A   33C    P0             24W /  250W |       3MiB /  16384MiB |      0%   E. Process |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|  No running processes found                                                             |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KDNRs6nJ8oWc"
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/stabilityai/stable-cascade/discussions/11\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "391e536878b84cfbb5773d86e668a8e6"
     ]
    },
    "id": "3_470t5d8oWc",
    "outputId": "876661ac-0fa1-41b7-8d32-13027ba7e910"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34e3f464bd2478797ffb03133d52dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SZkMqK8E8oWe"
   },
   "outputs": [],
   "source": [
    "def generate_prompt(context, question):\n",
    "    prompt_template = \"\"\"\n",
    "You are an expert in understanding and interpreting provided text contexts. Given a context and a question, your task is to generate an accurate and informative answer based on the provided context. Here is the structure:\n",
    "\n",
    "1. **Context:** The detailed text or passage that contains the information needed to answer the question.\n",
    "2. **Question:** A specific question that needs to be answered based on the context.\n",
    "\n",
    "Please make sure your response is clear, concise, and directly addresses the question. If the context does not contain sufficient information to answer the question, say I don't know.\n",
    "\n",
    "**Context:**\n",
    "{context}\n",
    "\n",
    "**Question:**\n",
    "{question}\n",
    "\n",
    "The response is a valid JSON with fields `explanation` and `response`.\n",
    "\"\"\"\n",
    "    return prompt_template.format(context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xxeNO4hY8oWf"
   },
   "outputs": [],
   "source": [
    "def AskLLM(context, question):\n",
    "    prompt = generate_prompt(context, question)\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    answer = tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "    # Extract the answer\n",
    "    json_match = re.search(r'{.*}', answer, re.DOTALL)\n",
    "\n",
    "    if json_match:\n",
    "        # extract and parse JSON\n",
    "        json_string = json_match.group(0)\n",
    "        response_dict = json.loads(json_string)\n",
    "        final_answer = response_dict['response']\n",
    "    else:\n",
    "        final_answer = \"I don't know.\"\n",
    "\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jIB6K54U8oWf",
    "outputId": "45279ee2-6e60-4ccc-b326-e6a8f4d42de6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      " 33%|███▎      | 8/24 [07:14<13:34, 50.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer not found for question: How would I describe Data Science in the following sentence:\n",
      "Based on the provided context, I would describe Data Science as \"an applied field growing out of traditional statistics\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [28:50<00:00, 72.11s/it]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "# generate ground truths for test set\n",
    "test_df['Final_answer'] = test_df.progress_apply(lambda row: AskLLM(row.raw_text, row.question), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "CnTBaqMo8oWg"
   },
   "outputs": [],
   "source": [
    "# drop raw text since it's present in other dataset file\n",
    "test_df = test_df.drop(columns='raw_text')\n",
    "\n",
    "# save to csv files\n",
    "test_df.to_csv(DatasetRoot + 'q_a_testset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6EQSjgs8oWg",
    "outputId": "dd938118-8344-4570-a120-dd4541a41659"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 44/96 [50:16<1:07:43, 78.15s/it]"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "# generate ground truths for train set\n",
    "train_df['Final_answer'] = train_df.progress_apply(lambda row: AskLLM(row.raw_text, row.question), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pX6nKhfE8oWi"
   },
   "outputs": [],
   "source": [
    "# drop raw text since it's present in other dataset file\n",
    "train_df = train_df.drop(columns='raw_text')\n",
    "\n",
    "# save to csv files\n",
    "train_df.to_csv(DatasetRoot + 'q_a_trainset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZ85izkw8oWi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "condaenv8724",
   "language": "python",
   "name": "condaenv8724"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
